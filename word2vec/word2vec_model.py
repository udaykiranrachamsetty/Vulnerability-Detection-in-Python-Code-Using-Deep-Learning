import os
import sys
from gensim.models import Word2Vec
import nltk


def load_corpus(file_path):
    with open(file_path, 'r', encoding='utf-8') as file:
        return file.read().lower().replace('\n', ' ')


def nltk_tokenize(text):
    nltk.download('punkt', quiet=True)
    all_sentences = nltk.sent_tokenize(text)
    return [nltk.word_tokenize(sentence) for sentence in all_sentences]


def train_word2vec_model(tokenized_data, min_count, iterations, size, mode):
    print(f"\n\n{mode} W2V model with min count {min_count}, {iterations} iterations, and size {size}")
    model_dir = f"word2vecnltk"
    model_file_name = f"{model_dir}/word2vec_{mode}{min_count}-{iterations}-{size}.model"

    # Ensure the directory exists
    if not os.path.exists(model_dir):
        os.makedirs(model_dir)
        print(f"Directory '{model_dir}' was created, as it did not exist.")

    model = Word2Vec(tokenized_data, vector_size=size, min_count=min_count, epochs=iterations, workers=4)
    model.save(model_file_name)
    print(f"Model saved as {model_file_name}")



def main():
    mode = sys.argv[1] if len(sys.argv) > 1 else "withString"
    print(f"Loading {mode}")

    file_path = f'pythontraining_{mode}_X'

    # Process and tokenize data
    print("Now processing...")
    processed_data = nltk_tokenize(load_corpus(file_path))
    print(f"Data has been tokenized. Proceeding to model training...")

    # Parameters for the model
    min_count = 10
    iterations = 100
    size = 200

    # Train the Word2Vec model directly with the tokenized data
    model = train_word2vec_model(processed_data, min_count, iterations, size, mode)


if __name__ == "_main_":
    main()
