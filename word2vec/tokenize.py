import sys
import subprocess
import io


def tokenize_python_file(input_file):
    try:
        p = subprocess.Popen(["python", "-m", "tokenize", input_file], stdout=subprocess.PIPE)
        out, err = p.communicate()
        return out.decode('utf-8')
    except subprocess.CalledProcessError as e:
        print("Error executing 'python -m tokenize':", str(e))
        return None


def process_tokenized_data(tokenized_data, mode):
    pythondata = ""
    count = 0
    totalcount = 0
    part = 0
    s = io.StringIO(tokenized_data)

    for line in s:
        totalcount += 1
        count += 1
        if totalcount % 1000 == 0:
            print(totalcount)

        position1 = line.find(":") + 1
        position2 = line.find("'")
        cat = line[position1:position2]
        content = line[position2 + 1:-2]

        if '"""' in line or "COMMENT" in cat:
            continue

        if mode == "withoutString" and "STRING" in cat:
            content = "\"string\""

        if "NL" in cat or "NEWLINE" in cat:
            pythondata += "\n"
        elif "INDENT" in cat:
            pythondata += "  " * content.count('t')
        else:
            pythondata += " " + content

        if count > 1000000:
            save_data(pythondata, mode, part)
            pythondata = ""
            part += 1
            count = 0

    save_data(pythondata, mode, part)
    return part


def save_data(data, mode, part):
    with open('pythontraining_{}_{}.txt'.format(mode, part), 'w') as outfile:
        outfile.write(data)
    print("Saved part " + str(part) + " (" + mode + ")")


def merge_files(mode, parts):
    fulltext = ""
    for i in range(parts):
        with open("pythontraining_{}_{}.txt".format(mode, i), "r") as infile:
            contents = infile.read()
            fulltext += contents
            print("Loaded part " + str(i))

    with open('pythontraining_{}_merged.txt'.format(mode), 'w') as outfile:
        outfile.write(fulltext)
    print("Merged files into 'pythontraining_{}_merged.txt'".format(mode))


if __name__ == "__main__":
    if len(sys.argv) > 1:
        mode = sys.argv[1]
    else:
        mode = "withString"

    input_file = "pythontraining_edit.txt"
    tokenized_data = tokenize_python_file(input_file)

    if tokenized_data is not None:
        part = process_tokenized_data(tokenized_data, mode)
        merge_files(mode, part + 1)
