import sys
import subprocess
import io

def tokenize_python_file(input_file):
    try:
        # Use the python tokenizer to tokenize the words in the corpus
        p = subprocess.Popen(["python", "-m", "tokenize", 'pythontraining_edit.txt'], stdout=subprocess.PIPE)
        out, err = p.communicate()

        return out.decode('utf-8')
    except subprocess.CalledProcessError as e:
        print("Error executing 'python -m tokenize':", str(e))
        return None

def process_tokenized_data(tokenized_data, mode="withString"):
    pythondata = ""

    count = 0
    totalcount = 0
    comment = 0
    part = 0

    s = io.StringIO(tokenized_data)

    for line in s:
        totalcount += 1
        count += 1
        if totalcount % 1000 == 0:
            print(totalcount)

        position1 = line.find(":") + 1
        position2 = line.find("'")
        position3 = line[position2 + 1:].find("'")

        cat = line[position1:position2]
        content = line[position2 + 1:-2]

        if '"""' in line or "COMMENT" in cat:
            comment += 1
            continue

        if mode == "withoutString" and "STRING" in cat:
            stringstart = line.find("\"")
            content = "\"string\""

        if "NL" in cat or "NEWLINE" in cat:
            pythondata += "\n"
        elif "INDENT" in cat:
            pythondata += "  " * content.count('t')
        else:
            pythondata += " " + content

        # Save in parts to reduce computational load
        if count > 1000000:
            print("saving part " + str(part) + " (" + mode + ") " + str(totalcount))
            with open('pythontraining_{}_{}.txt'.format(mode, part), 'w') as outfile:
                outfile.write(pythondata)
            pythondata = ""
            part += 1
            count = 0

    with open('pythontraining_{}_{}.txt'.format(mode, part), 'w') as outfile:
        outfile.write(pythondata)

if __name__ == "__main__":
    if len(sys.argv) > 1:
        mode = sys.argv[1]
    else:
        mode = "withString"  # Default mode

    input_file = "pythontraining_edit.txt"
    tokenized_data = tokenize_python_file(input_file)

    if tokenized_data is not None:
        process_tokenized_data(tokenized_data, mode)
