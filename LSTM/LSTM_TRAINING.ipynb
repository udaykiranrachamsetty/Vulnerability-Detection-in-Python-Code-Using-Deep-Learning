{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "V100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Testing Word2Vec Model"
      ],
      "metadata": {
        "id": "4AA1AQ4VzCHt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O0I8cDaJxHCi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2c21ed93-4e37-4c94-cea9-65ccad978cbb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q gensim\n",
        "import gensim\n",
        "model = gensim.models.Word2Vec.load(\"/content/drive/MyDrive/Colab Notebooks/word2vecmodel/word2vec_withString10-100-200.model\")\n"
      ],
      "metadata": {
        "id": "a7D5Kd4DxV3K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gensim.__version__"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "-RTXl52cf6dm",
        "outputId": "d1197a9c-0ef0-432e-877f-c43f612f132b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'4.3.2'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "similar_words = model.wv.most_similar(\"if\")\n",
        "for word, similarity in similar_words:\n",
        "  print(f\"{word}: {similarity}\")\n"
      ],
      "metadata": {
        "id": "m2kqeZKBx64V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vector = model.wv[\"if\"]\n",
        "print(vector)"
      ],
      "metadata": {
        "id": "1rwLludSJa9T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset Preprocessing"
      ],
      "metadata": {
        "id": "aGS56TEG17BP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "import os\n",
        "sys.path.append(os.path.dirname('/content/drive/MyDrive/Colab Notebooks/myutils.py'))\n",
        "import json\n",
        "from datetime import datetime\n",
        "import random\n",
        "import pickle\n",
        "import numpy as np\n",
        "from gensim.models import Word2Vec, KeyedVectors\n"
      ],
      "metadata": {
        "id": "z28YgfB3zPJQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import myutils"
      ],
      "metadata": {
        "id": "4nMWhjPd3Hr0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mode = \"sql\"\n",
        "mincount = 10\n",
        "iterationen = 100\n",
        "s = 200\n",
        "w = \"withString\"\n",
        "\n",
        "w2vmodel = f\"/content/drive/MyDrive/Colab Notebooks/word2vecmodel/word2vec_withString10-100-200.model\"\n",
        "\n",
        "if not os.path.isfile(w2vmodel):\n",
        "    print(\"word2vec model is still being created...\")\n",
        "    sys.exit()\n",
        "\n",
        "w2v_model = Word2Vec.load(w2vmodel)\n",
        "word_vectors = w2v_model.wv\n"
      ],
      "metadata": {
        "id": "Sm6w74gyz_g1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "file_path = f'/content/drive/MyDrive/Colab Notebooks/datasets/plain_{mode}'\n",
        "with open(file_path, 'r') as infile:\n",
        "    data = json.load(infile)\n",
        "\n",
        "print(\"Data loaded successfully.\")\n"
      ],
      "metadata": {
        "id": "lKHDzzRO0RDH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6d0fb0fa-d908-45ac-9718-731c303b9974"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data loaded successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "allblocks = []\n",
        "repos = 0\n",
        "files = 0\n",
        "commits = 0\n",
        "sourcelength = 0\n",
        "for r in data:\n",
        "    repos += 1\n",
        "    for c in data[r]:\n",
        "        commits += 1\n",
        "        if \"files\" in data[r][c]:\n",
        "            for f in data[r][c][\"files\"]:\n",
        "                files += 1\n",
        "                if not \"source\" in data[r][c][\"files\"][f]:\n",
        "                    continue\n",
        "                sourcecode = data[r][c][\"files\"][f][\"source\"]\n",
        "                sourcelength += len(sourcecode)\n",
        "                allbadparts = []\n",
        "                for change in data[r][c][\"files\"][f][\"changes\"]:\n",
        "                    badparts = change[\"badparts\"]\n",
        "                    for bad in badparts:\n",
        "                        pos = myutils.findposition(bad, sourcecode)\n",
        "                        if not -1 in pos:\n",
        "                            allbadparts.append(bad)\n",
        "                if len(allbadparts) > 0:\n",
        "                    positions = myutils.findpositions(allbadparts, sourcecode)\n",
        "                    blocks = myutils.getblocks(sourcecode, positions, step=5, fulllength=200)\n",
        "                    for b in blocks:\n",
        "                        allblocks.append(b)\n",
        "\n",
        "print(f\"Total number of repositories: {repos}\")\n",
        "print(f\"Total number of files: {files}\")\n",
        "print(f\"Total number of commits: {commits}\")\n",
        "print(\"Data processing complete.\")"
      ],
      "metadata": {
        "id": "ke7BGrVJ1S0P",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "64315525-e9ff-4ea2-e229-8a6584486e36"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of repositories: 336\n",
            "Total number of files: 667\n",
            "Total number of commits: 406\n",
            "Data processing complete.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(sourcelength)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JD6ClSyIAeOC",
        "outputId": "70c7b1cb-220d-4abd-a302-74a750af459e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2912035\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(allblocks)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oC1pPmUk0aY5",
        "outputId": "81d8b4c8-a16a-43c4-babc-2a0d6e4753b9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "284599"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: save that allblocks in a file\n",
        "allblocks = []\n",
        "with open(\"/content/drive/MyDrive/Colab Notebooks/allblocks.pkl\", \"wb\") as f:\n",
        "    pickle.dump(allblocks, f)\n"
      ],
      "metadata": {
        "id": "JWqCoZtBtdJl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: with open(\"/content/drive/MyDrive/Colab Notebooks/allblocks.pkl\", \"wb\") as f:     pickle.dump(allblocks, f)  can you read the same file and load into allblocks\n",
        "\n",
        "with open(\"/content/drive/MyDrive/Colab Notebooks/allblocks.pkl\", \"rb\") as f:\n",
        "    allblocks = pickle.load(f)\n"
      ],
      "metadata": {
        "id": "uhewxiFwJ8pB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Shuffle and split the data into train, validate, and final test sets\n",
        "keys = list(range(len(allblocks)))\n",
        "random.shuffle(keys)\n",
        "\n",
        "cutoff = round(0.7 * len(keys))  # 70% for the training set\n",
        "cutoff2 = round(0.85 * len(keys))  # 15% for the validation set, 15% for the final test set\n",
        "\n",
        "keystrain = keys[:cutoff]\n",
        "keystest = keys[cutoff:cutoff2]\n",
        "keysfinaltest = keys[cutoff2:]\n",
        "\n",
        "# Save the keys for later use\n",
        "with open('/content/drive/MyDrive/Colab Notebooks/datasetPreProcessed/' + mode + '_dataset_keystrain', 'wb') as fp:\n",
        "    pickle.dump(keystrain, fp)\n",
        "with open('/content/drive/MyDrive/Colab Notebooks/datasetPreProcessed/' + mode + '_dataset_keystest', 'wb') as fp:\n",
        "    pickle.dump(keystest, fp)\n",
        "with open('/content/drive/MyDrive/Colab Notebooks/datasetPreProcessed/' + mode + '_dataset_keysfinaltest', 'wb') as fp:\n",
        "    pickle.dump(keysfinaltest, fp)\n",
        "\n",
        "print(f\"Train/Test/Final Test split: {len(keystrain)}/{len(keystest)}/{len(keysfinaltest)}\")\n"
      ],
      "metadata": {
        "id": "LeCtaMx21ZBw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import gc\n",
        "def batch_processor(allblocks, keys, batch_size=100):\n",
        "    for i in range(0, len(keys), batch_size):\n",
        "        batch_keys = keys[i:i + batch_size]\n",
        "        batch_vectors = []\n",
        "        batch_labels = []\n",
        "        for k in batch_keys:\n",
        "            block = allblocks[k]\n",
        "            code = block[0]\n",
        "            token = myutils.getTokens(code)\n",
        "            vectorlist = []\n",
        "            for t in token:\n",
        "                if t in word_vectors.key_to_index and t != \" \":\n",
        "                    vector = w2v_model.wv[t]\n",
        "                    vectorlist.append(vector)\n",
        "            if vectorlist:\n",
        "                batch_vectors.append(np.stack(vectorlist))\n",
        "                batch_labels.append(block[1])\n",
        "        gc.collect()\n",
        "        yield batch_vectors, batch_labelsa\n",
        "\n",
        "TrainX, TrainY = [], []\n",
        "\n",
        "for vectors, labels in batch_processor(allblocks, keystrain):\n",
        "    TrainX.extend(vectors)\n",
        "    TrainY.extend(labels)\n",
        "    gc.collect()\n"
      ],
      "metadata": {
        "id": "PLEg9cu8wKSe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(type(TrainX))"
      ],
      "metadata": {
        "id": "WvoHsRW5XIDi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Train length: \" + str(len(TrainX)))\n",
        "print(\"Train length: \" + str(len(TrainY)))"
      ],
      "metadata": {
        "id": "IcOX15yoP_pH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(TrainX[0])"
      ],
      "metadata": {
        "id": "HjbLxlQ2cDW_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"/content/drive/MyDrive/Colab Notebooks/datasetPreProcessed/\" + mode + \"_TrainY\", \"wb\") as f:\n",
        "    pickle.dump(TrainY, f)\n"
      ],
      "metadata": {
        "id": "Fgi4jMd8hMqA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"/content/drive/MyDrive/Colab Notebooks/datasetPreProcessed/\" + mode + \"_TrainY\", \"rb\") as f:\n",
        "    TrainY = pickle.load(f)\n"
      ],
      "metadata": {
        "id": "lsRJxZLFkT7f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ValidateX, ValidateY = [], []\n",
        "\n",
        "for vectors, labels in batch_processor(allblocks, keystest):\n",
        "    ValidateX.extend(vectors)\n",
        "    ValidateY.extend(labels)\n",
        "    gc.collect()\n",
        "\n",
        "print(\"Validate length: \" + str(len(ValidateX)))\n",
        "print(\"Validate length: \" + str(len(ValidateY)))\n",
        "\n",
        "with open(\"/content/drive/MyDrive/Colab Notebooks/datasetPreProcessed/\" + mode + \"_ValidateX\", \"wb\") as f:\n",
        "    pickle.dump(ValidateX, f)\n"
      ],
      "metadata": {
        "id": "TiIV5uhfQzcQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "with open(\"/content/drive/MyDrive/Colab Notebooks/datasetPreProcessed/\" + mode + \"_ValidateY\", \"wb\") as f:\n",
        "    pickle.dump(ValidateY, f)\n"
      ],
      "metadata": {
        "id": "HQsTVlFphR8V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "with open(\"/content/drive/MyDrive/Colab Notebooks/datasetPreProcessed/\" + mode + \"_ValidateX\", \"rb\") as f:\n",
        "  ValidateX = pickle.load(f)\n",
        "\n",
        "\n",
        "with open(\"/content/drive/MyDrive/Colab Notebooks/datasetPreProcessed/\" + mode + \"_ValidateY\", \"rb\") as f:\n",
        "  ValidateY = pickle.load(f)\n"
      ],
      "metadata": {
        "id": "yKmvUlNokYx0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "FinaltestX, FinaltestY = [], []\n",
        "\n",
        "for vectors, labels in batch_processor(allblocks, keysfinaltest):\n",
        "    FinaltestX.extend(vectors)\n",
        "    FinaltestY.extend(labels)\n",
        "\n",
        "    gc.collect()\n",
        "\n",
        "print(\"Finaltest length: \" + str(len(FinaltestX)))\n",
        "print(\"Finaltest length: \" + str(len(FinaltestY)))\n",
        "\n",
        "with open(\"/content/drive/MyDrive/Colab Notebooks/datasetPreProcessed/\" + mode + \"_FinaltestX\", \"wb\") as f:\n",
        "    pickle.dump(FinaltestX, f)\n"
      ],
      "metadata": {
        "id": "D64dCjxgBbNU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "with open(\"/content/drive/MyDrive/Colab Notebooks/datasetPreProcessed/\" + mode + \"_FinaltestY\", \"wb\") as f:\n",
        "    pickle.dump(FinaltestY, f)\n"
      ],
      "metadata": {
        "id": "BRs5DaSqhXCv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "with open(\"/content/drive/MyDrive/Colab Notebooks/datasetPreProcessed/\" + mode + \"_FinaltestX\", \"rb\") as f:\n",
        "  FinaltestX = pickle.load(f)\n",
        "\n",
        "\n",
        "with open(\"/content/drive/MyDrive/Colab Notebooks/datasetPreProcessed/\" + mode + \"_FinaltestY\", \"rb\") as f:\n",
        "  FinaltestY = pickle.load(f)\n"
      ],
      "metadata": {
        "id": "XUy1YS2qkiv8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Prepare Dataset for LSTM Model"
      ],
      "metadata": {
        "id": "FVs-z8eThnWW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "len(TrainX)"
      ],
      "metadata": {
        "id": "SM9iUSleh5YE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(TrainX[199215])"
      ],
      "metadata": {
        "id": "dmSNASnbizHE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "\n",
        "X_train = pad_sequences(TrainX, padding='post', dtype='float32')\n",
        "X_validate = pad_sequences(ValidateX, padding='post', dtype='float32')\n",
        "X_finaltest = pad_sequences(FinaltestX, padding='post', dtype='float32')\n",
        "\n",
        "\n",
        "y_train = np.array(TrainY)\n",
        "y_validate = np.array(ValidateY)\n",
        "\n",
        "y_finaltest = np.array(FinaltestY)\n"
      ],
      "metadata": {
        "id": "LwAFkG-IhsUF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(len(y_train)):\n",
        "  if y_train[i] == 0:\n",
        "    y_train[i] = 1\n",
        "  else:\n",
        "    y_train[i] = 0\n",
        "\n",
        "for i in range(len(y_validate)):\n",
        "  if y_validate[i] == 0:\n",
        "    y_validate[i] = 1\n",
        "  else:\n",
        "    y_validate[i] = 0\n",
        "\n",
        "for i in range(len(y_finaltest)):\n",
        "  if y_finaltest[i] == 0:\n",
        "    y_finaltest[i] = 1\n",
        "  else:\n",
        "    y_finaltest[i] = 0"
      ],
      "metadata": {
        "id": "RUcaY8GElmTB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fulllength = 200\n",
        "now = datetime.now()\n",
        "nowformat = now.strftime(\"%H:%M\")\n",
        "print(\"numpy array done. \", nowformat)\n",
        "\n",
        "print(str(len(X_train)) + \" samples in the training set.\")\n",
        "print(str(len(X_validate)) + \" samples in the validation set.\")\n",
        "print(str(len(X_finaltest)) + \" samples in the final test set.\")\n",
        "\n",
        "csum = 0\n",
        "for a in y_train:\n",
        "  csum = csum+a\n",
        "print(\"percentage of vulnerable samples: \"  + str(int((csum / len(X_train)) * 10000)/100) + \"%\")\n",
        "\n",
        "testvul = 0\n",
        "for y in y_validate:\n",
        "  if y == 1:\n",
        "    testvul = testvul+1\n",
        "print(\"absolute amount of vulnerable samples in test set: \" + str(testvul))\n",
        "\n",
        "max_length = fulllength"
      ],
      "metadata": {
        "id": "upTu3-bxmBTB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#padding sequences on the same length\n",
        "X_train = pad_sequences(X_train, maxlen=max_length)\n",
        "X_test = pad_sequences(X_validate, maxlen=max_length)\n",
        "X_finaltest = pad_sequences(X_finaltest, maxlen=max_length)"
      ],
      "metadata": {
        "id": "0MIl024conuB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: way to save x_train and so on\n",
        "\n",
        "import numpy as np\n",
        "# Save the preprocessed data\n",
        "np.save('/content/drive/MyDrive/Colab Notebooks/datasetPreProcessed/' + mode + '_X_train.npy', X_train)\n",
        "np.save('/content/drive/MyDrive/Colab Notebooks/datasetPreProcessed/' + mode + '_X_validate.npy', X_validate)\n",
        "np.save('/content/drive/MyDrive/Colab Notebooks/datasetPreProcessed/' + mode + '_X_finaltest.npy', X_finaltest)\n",
        "np.save('/content/drive/MyDrive/Colab Notebooks/datasetPreProcessed/' + mode + '_y_train.npy', y_train)\n",
        "np.save('/content/drive/MyDrive/Colab Notebooks/datasetPreProcessed/' + mode + '_y_validate.npy', y_validate)\n",
        "np.save('/content/drive/MyDrive/Colab Notebooks/datasetPreProcessed/' + mode + '_y_finaltest.npy', y_finaltest)\n"
      ],
      "metadata": {
        "id": "H6_-Nb2bmbMX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: load those saved numpy arrays\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "mode = \"sql\"  # Default mode/type of vulnerability\n",
        "mincount = 10  # Minimum times a word has to appear in the corpus\n",
        "iterationen = 100  # Training iterations for the word2vec model\n",
        "s = 200  # Dimensions of the word2vec model\n",
        "w = \"withString\"\n",
        "\n",
        "# Load the preprocessed data\n",
        "X_train = np.load('/content/drive/MyDrive/Colab Notebooks/datasetPreProcessed/' + mode + '_X_train.npy')\n",
        "X_validate = np.load('/content/drive/MyDrive/Colab Notebooks/datasetPreProcessed/' + mode + '_X_validate.npy')\n",
        "X_finaltest = np.load('/content/drive/MyDrive/Colab Notebooks/datasetPreProcessed/' + mode + '_X_finaltest.npy')\n",
        "y_train = np.load('/content/drive/MyDrive/Colab Notebooks/datasetPreProcessed/' + mode + '_y_train.npy')\n",
        "y_validate = np.load('/content/drive/MyDrive/Colab Notebooks/datasetPreProcessed/' + mode + '_y_validate.npy')\n",
        "y_finaltest = np.load('/content/drive/MyDrive/Colab Notebooks/datasetPreProcessed/' + mode + '_y_finaltest.npy')\n"
      ],
      "metadata": {
        "id": "3iA51EPvt6PU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training LSTM Model"
      ],
      "metadata": {
        "id": "vw-cj8Ni5mL-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import keras\n",
        "keras.__version__"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "nLOooYn_gPRE",
        "outputId": "b5d1788a-365f-44aa-e8df-8355514a6378"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'2.15.0'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sklearn\n",
        "sklearn.__version__"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "1dbAKBxSgrRV",
        "outputId": "71c24823-ce24-412f-eaba-7b7edba67f36"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'1.2.2'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import LSTM, Dense\n",
        "from keras.preprocessing import sequence\n",
        "from sklearn.utils import class_weight\n",
        "import numpy as np\n",
        "\n",
        "dropout = 0.25\n",
        "neurons = 150\n",
        "optimizer = \"adam\"\n",
        "epochs = 200\n",
        "batch_size = 256\n",
        "max_length = 200\n",
        "s = 200\n",
        "\n",
        "now = datetime.now()\n",
        "nowformat = now.strftime(\"%H:%M\")\n",
        "print(\"Starting LSTM: \", nowformat)\n",
        "\n",
        "\n",
        "print(\"Dropout: \" + str(dropout))\n",
        "print(\"Neurons: \" + str(neurons))\n",
        "print(\"Optimizer: \" + optimizer)\n",
        "print(\"Epochs: \" + str(epochs))\n",
        "print(\"Batch Size: \" + str(batchsize))\n",
        "print(\"max length: \" + str(max_length))\n",
        "\n",
        "model = Sequential()\n",
        "model.add(LSTM(neurons, dropout=dropout, recurrent_dropout=dropout, input_shape=(max_length, s)))\n",
        "model.add(LSTM(neurons, dropout=dropout, recurrent_dropout=dropout, input_shape=(max_length, s)))\n",
        "model.add(LSTM(neurons, dropout=dropout, recurrent_dropout=dropout, input_shape=(max_length, s)))\n",
        "model.add(LSTM(neurons, dropout=dropout, recurrent_dropout=dropout, input_shape=(max_length, s)))\n",
        "model.add(LSTM(neurons, dropout=dropout, recurrent_dropout=dropout, input_shape=(max_length, s)))\n",
        "model.add(LSTM(neurons, dropout=dropout, recurrent_dropout=dropout, input_shape=(max_length, s)))\n",
        "model.add(LSTM(neurons, dropout=dropout, recurrent_dropout=dropout, input_shape=(max_length, s)))\n",
        "model.add(LSTM(neurons, dropout=dropout, recurrent_dropout=dropout, input_shape=(max_length, s)))\n",
        "model.add(LSTM(neurons, dropout=dropout, recurrent_dropout=dropout, input_shape=(max_length, s)))\n",
        "model.add(LSTM(neurons, dropout=dropout, recurrent_dropout=dropout, input_shape=(max_length, s)))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "model.compile(loss=myutils.f1_loss, optimizer=optimizer, metrics=[myutils.f1])\n",
        "\n",
        "model.summary()\n"
      ],
      "metadata": {
        "id": "PpUGZauP5png"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Adjust class weights for imbalanced datasets\n",
        "class_weights = class_weight.compute_class_weight('balanced', np.unique(y_train), y_train)\n"
      ],
      "metadata": {
        "id": "vJcFYevR5t4E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model\n",
        "history = model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, validation_data=(X_validate, y_validate), class_weight=class_weights)\n"
      ],
      "metadata": {
        "id": "S_a0ohCZ6I7i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "import numpy as np\n",
        "\n",
        "# Function to evaluate the model on a dataset\n",
        "def evaluate_model(dataset_name, X, y):\n",
        "    # For binary classification\n",
        "    yhat_probs = model.predict(X, verbose=0)\n",
        "    yhat_classes = (yhat_probs > 0.5).astype(\"int32\")\n",
        "\n",
        "    # Evaluate predictions\n",
        "    accuracy = accuracy_score(y, yhat_classes)\n",
        "    precision = precision_score(y, yhat_classes)\n",
        "    recall = recall_score(y, yhat_classes)\n",
        "    F1Score = f1_score(y, yhat_classes)\n",
        "\n",
        "    print(f\"Now predicting on {dataset_name} set:\")\n",
        "    print(f\"Accuracy: {accuracy}\")\n",
        "    print(f\"Precision: {precision}\")\n",
        "    print(f\"Recall: {recall}\")\n",
        "    print(f'F1 score: {F1Score}\\n')\n",
        "\n",
        "# Evaluate on each dataset\n",
        "evaluate_model(\"Train\", X_train, y_train)\n",
        "evaluate_model(\"Validation\", X_validate, y_validate)\n",
        "evaluate_model(\"Final Test\", X_finaltest, y_finaltest)\n"
      ],
      "metadata": {
        "id": "QuvPGjMs6L_c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "now = datetime.now() # current date and time\n",
        "nowformat = now.strftime(\"%H:%M\")"
      ],
      "metadata": {
        "id": "RBL7cEq36kBu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the model for later use\n",
        "model_path = '/content/drive/MyDrive/Models/LSTM_model_' + mode + '.h5'\n",
        "model.save(model_path)\n",
        "print(f'Model saved to {model_path}')\n"
      ],
      "metadata": {
        "id": "MCXpLIv06rYT"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}